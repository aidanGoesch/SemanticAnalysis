from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import numpy as np
from src.keys import HUGGING_FACE_TOKEN
import re

if torch.backends.mps.is_built():  # Apple Silicon
    print('mps is used.')
    mps_device = torch.device("mps")
elif torch.backends.cuda.is_built():  # Real GPU
    print('cuda is used.')
    mps_device = torch.device("cuda")
else:  # If all else fails
    print('cpu is used.')
    mps_device = torch.device("cpu")

# number of previous sentences used to calculate context dependent sequentiality
CALL_BACK = 4


class SequentialityModel:
    def __init__(self, model_name : str, topic : str) -> None:
        self.sentences = []

        self.tokenizer = AutoTokenizer.from_pretrained(model_name,
                                                       token=HUGGING_FACE_TOKEN,
                                                       use_safetensors=True,
                                                       padding_side="left")
        self.model = AutoModelForCausalLM.from_pretrained(model_name,
                                                          token=HUGGING_FACE_TOKEN,
                                                          torch_dtype=torch.bfloat16,
                                                          device_map=mps_device,
                                                          use_safetensors=True).to(mps_device)

        self.model.config.pad_token_id = self.model.config.eos_token_id

        # turn off gradient descent
        torch.set_grad_enabled(False)

        # Pad all text with _
        self.context_string = f"_condition every word on this topic: <TOPIC>{topic}<END_TOPIC> "

    def _to_tokens_and_logprobs(self, text : str) -> list[(str, float)]:
        """Function that takes a string and returns a list of tokens and logprobs associated with it
        had it been generated by the model."""
        input_text = self.context_string + text

        input_ids = self.tokenizer(input_text, padding=True, return_tensors="pt").input_ids.to(mps_device)
        outputs = self.model(input_ids)
        probs = torch.log_softmax(outputs.logits, dim=-1).detach()

        # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1
        probs = probs[:, :-1, :]
        input_ids = input_ids[:, 1:]
        gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)

        batch = []
        for input_sentence, input_probs in zip(input_ids, gen_probs):
            text_sequence = []
            for token, p in zip(input_sentence, input_probs):
                if token not in self.tokenizer.all_special_ids:
                    text_sequence.append((self.tokenizer.decode(token), p.item()))
            batch.append(text_sequence)
        return batch

    @staticmethod
    def _process_tokens_and_logprobs(query_tokens: list[str], tokens_and_logprobs: list[(str, float)]) -> float:
        """Function that returns the sum of logprobs of the tokens in the last sentence"""
        i = 0
        while i < len(tokens_and_logprobs):
            if tokens_and_logprobs[i][0] == query_tokens[0]:  # if the first token in the sentence matches the cursor
                for j in range(len(query_tokens)):  # start iterating through the query list to see if it matches
                    if tokens_and_logprobs[i + j][0] != query_tokens[j]:
                        i = i + j
                        break
                else:
                    print(tokens_and_logprobs[i:])
                    return sum(map(lambda x: x[1], tokens_and_logprobs[i:]))

            i += 1

        print("ERROR")
        print("query:", query_tokens)
        print("logprobs:", tokens_and_logprobs)
        return 0

    def calculate_contextual_sequentiality(self, sentence : str, sentence_tokens : list[str], i : int,  h : int, verbose : bool = False) -> float:
        """Calculate the contextually dependent sequentiality of a sentence."""
        raw_sequentiality = 0
        if i - h < 0:
            context = ". ".join(self.sentences[:i])

        else:
            context = ". ".join(self.sentences[i - h:i])

        if len(context) == 0:  # beginning of the text - prevents random period at the front of the text
            input_text = sentence + "."
        else:
            input_text = context + ". " + sentence + "."

        tokens_and_logprobs = self._to_tokens_and_logprobs(input_text)[0]

        return self._process_tokens_and_logprobs(sentence_tokens, tokens_and_logprobs)

    def calculate_topic_sequentiality(self, sentence : str, sentence_tokens : list[str], verbose : bool = False) -> float:
        """Calculate the sequentiality of a sentence given only a topic"""
        tokens_and_logprobs = self._to_tokens_and_logprobs(sentence + ".")[0]

        return self._process_tokens_and_logprobs(sentence_tokens, tokens_and_logprobs)


    def calculate_sequentiality(self, sentence : str, i: int, verbose : bool = False) -> float:
        """Calculates the sequentiality of a given sentence by subtracting the context dependent sequentiality from
        the purely topic driven version."""
        sentence_tokens = [self.tokenizer.decode(x) for x in self.tokenizer.encode(sentence)]
        print(sentence_tokens)

        topic_sequentiality = self.calculate_topic_sequentiality(sentence, sentence_tokens)
        contextual_sequentiality = self.calculate_contextual_sequentiality(sentence, sentence_tokens, i, CALL_BACK, False)

        return (contextual_sequentiality - topic_sequentiality) / len(sentence_tokens)  # flipped because both functions return NLL already

    def calculate_total_sequentiality(self, text : str, verbose : bool = False) -> float:
        self.sentences = re.split('[\.\?\!]\s*', text)
        sequentialities = []

        for i, sentence in enumerate(self.sentences):
            if sentence == "": break

            sequentialities.append(self.calculate_sequentiality(sentence, i, False))

        print(sequentialities)
        return np.mean(sequentialities)


if __name__ == "__main__":
    model = SequentialityModel("microsoft/Phi-3-mini-4k-instruct", topic="a conversation with a doctor")
    print(f"\nshould be lower  : {model.calculate_total_sequentiality("There are two bison standing next to each other. They seem to be friends.", False)}")
    print(f"\nshould be higher : {model.calculate_total_sequentiality("I broke my arm. It hurts a lot, and I don't know if it'll ever heal.", False)}")

    # tmp = model._to_tokens_and_logprobs("")[0]
    #
    # print(tmp)
    #
    # print("control:", sum(map(lambda x: x[1], tmp)))
    #
    # import time
    # start = time.time()
    # print(f"\ntotal NLL of test scene= {model.calculate_total_sequentialty('"THE BOYFRIEND" in bold white text fades in on a black screen before fading out. The letters of "high maintenance" appear in the center of the screen one by one in white text. A simple jingle plays in the background.', False)}")
    # print("time to run: ", time.time() - start)
